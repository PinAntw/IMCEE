import json
import numpy as np
from transformers import AutoTokenizer
import matplotlib.pyplot as plt
import seaborn as sns

# ================= è¨­å®šå€ =================
DATA_PATH = '/home/joung/r13725060/Research/IMCEE/data/preprocess/conversations.jsonl'
MODEL_NAME = 'roberta-base'  # æˆ– 'roberta-large'ï¼Œè¦–æ‚¨ä½¿ç”¨çš„æ¨¡åž‹è€Œå®š
MAX_LEN = 512
# =========================================

def analyze_token_length():
    print(f"Loading tokenizer: {MODEL_NAME}...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    
    lengths = []
    truncated_count = 0
    total_count = 0
    
    print(f"Reading data from {DATA_PATH}...")
    
    with open(DATA_PATH, 'r', encoding='utf-8') as f:
        for line in f:
            data = json.loads(line)
            utterances = data['utterances']
            
            # æ¨¡æ“¬ Encoder 2 çš„è¡Œç‚ºï¼šå°‡æ‰€æœ‰å¥å­æ‹¼åœ¨ä¸€èµ·
            # æ³¨æ„ï¼šRoBERTa å¯¦éš›ä¸Šæœƒæ˜¯ [CLS] utt1 [SEP] utt2 [SEP] ...
            # æˆ‘å€‘é€™è£¡ç”¨ç°¡å–®çš„æ‹¼æŽ¥ä¾†ä¼°ç®—ï¼Œèª¤å·®æ¥µå° (ä¸»è¦çœ‹ subword æ•¸é‡)
            full_text = " ".join([u['text'] for u in utterances])
            
            # è¨ˆç®— Token æ•¸é‡ (åŒ…å« special tokens)
            token_ids = tokenizer.encode(full_text, add_special_tokens=True)
            length = len(token_ids)
            
            lengths.append(length)
            total_count += 1
            
            if length > MAX_LEN:
                truncated_count += 1

    # --- çµ±è¨ˆå ±å‘Š ---
    lengths = np.array(lengths)
    
    print("\n" + "="*40)
    print(f"ðŸ“Š Token Length Statistics (Model: {MODEL_NAME})")
    print("="*40)
    print(f"Total Conversations : {total_count}")
    print(f"Truncated Samples   : {truncated_count} ({truncated_count/total_count*100:.2f}%)")
    print(f"Safe Samples (<{MAX_LEN}) : {total_count - truncated_count} ({(total_count - truncated_count)/total_count*100:.2f}%)")
    print("-" * 40)
    print(f"Min Length          : {np.min(lengths)}")
    print(f"Mean Length         : {np.mean(lengths):.2f}")
    print(f"Median Length       : {np.median(lengths):.2f}")
    print(f"Max Length          : {np.max(lengths)}")
    print("-" * 40)
    print(f"75th Percentile     : {np.percentile(lengths, 75):.2f}")
    print(f"90th Percentile     : {np.percentile(lengths, 90):.2f}")
    print(f"95th Percentile     : {np.percentile(lengths, 95):.2f}")
    print(f"99th Percentile     : {np.percentile(lengths, 99):.2f}")
    print("="*40)

    # (é¸ç”¨) ç•«åœ–
    try:
        plt.figure(figsize=(10, 6))
        sns.histplot(lengths, bins=50, kde=True)
        plt.axvline(x=MAX_LEN, color='r', linestyle='--', label=f'Limit ({MAX_LEN})')
        plt.title(f'Token Length Distribution ({MODEL_NAME})')
        plt.xlabel('Token Count')
        plt.ylabel('Frequency')
        plt.legend()
        plt.savefig('length_distribution.png')
        print("Histrogram saved to 'length_distribution.png'")
    except:
        print("Skipping plot generation (matplotlib/seaborn missing or display issue).")

if __name__ == "__main__":
    analyze_token_length()
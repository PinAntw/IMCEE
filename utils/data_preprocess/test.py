import pandas as pd
import re
import os
from collections import defaultdict

# ================= è¨­å®šå€ =================
# è«‹ä¿®æ”¹é€™è£¡æŒ‡å‘ä½ çš„ CSV è³‡æ–™å¤¾è·¯å¾‘
BASE_CSV_DIR = '/home/joung/r13725060/Research/RECCON/data/subtask2/fold1'

# è¦æª¢æŸ¥çš„æª”æ¡ˆæ¸…å–®
TARGET_FILES = [
    'dailydialog_classification_valid_without_context.csv',
]

# ç”¨ä¾†è§£æ ID çš„æ­£å‰‡è¡¨é”å¼
id_pattern = re.compile(r'dailydialog_(?P<split>\w+)_(?P<conv_num>\d+)_utt_(?P<t_idx>\d+)_(?:.*)_cause_utt_(?P<c_idx>\d+)(?:_span_\d+)?')

def analyze_merges(csv_path):
    print(f"\n{'='*60}")
    print(f"æ­£åœ¨åˆ†ææª”æ¡ˆ: {os.path.basename(csv_path)}")
    print(f"{'='*60}")

    if not os.path.exists(csv_path):
        print(f"âŒ æ‰¾ä¸åˆ°æª”æ¡ˆ: {csv_path}")
        return

    df = pd.read_csv(csv_path)
    
    # å­—å…¸çµæ§‹: Key=(conv_id, t_utt_id, c_utt_id), Value=List of rows
    pair_groups = defaultdict(list)
    parse_errors = 0

    # 1. åˆ†çµ„
    for index, row in df.iterrows():
        raw_id = row['id']
        span_text = row.get('span', 'N/A') # æœ‰äº›ç‰ˆæœ¬å¯èƒ½æ¬„ä½åä¸åŒ
        label = row['labels']

        match = id_pattern.search(raw_id)
        if not match:
            parse_errors += 1
            continue

        split_prefix = match.group('split')
        conv_num = match.group('conv_num')
        t_idx = match.group('t_idx')
        c_idx = match.group('c_idx')

        # é€™æ˜¯å”¯ä¸€è­˜åˆ¥ä¸€çµ„ Pair çš„ Key
        unique_key = (f"{split_prefix}_{conv_num}", f"u{t_idx}", f"u{c_idx}")
        
        # å„²å­˜è©²è¡Œçš„è³‡è¨Š
        pair_groups[unique_key].append({
            "id": raw_id,
            "span": span_text,
            "label": label
        })

    # 2. çµ±è¨ˆ
    total_raw_rows = len(df)
    unique_pairs_count = len(pair_groups)
    merged_groups = {k: v for k, v in pair_groups.items() if len(v) > 1}
    merged_count = len(merged_groups)
    rows_eliminated = total_raw_rows - unique_pairs_count - parse_errors

    # 3. è¼¸å‡ºå ±å‘Š
    print(f"ğŸ“Š çµ±è¨ˆæ•¸æ“š:")
    print(f"  - åŸå§‹ CSV ç¸½è¡Œæ•¸: {total_raw_rows}")
    print(f"  - å”¯ä¸€ Pair æ•¸é‡ (JSONL æœ€çµ‚æ•¸é‡): {unique_pairs_count}")
    print(f"  - åŒ…å«å¤šå€‹ Span çš„é‡è¤‡çµ„æ•¸: {merged_count} çµ„")
    print(f"  - å› åˆä½µæ¸›å°‘çš„è¡Œæ•¸: {rows_eliminated}")
    if parse_errors > 0:
        print(f"  - Regex è§£æå¤±æ•—: {parse_errors}")

    # 4. å°å‡ºè©³ç´°ç¯„ä¾‹ (å‰ 3 çµ„)
    if merged_count > 0:
        print(f"\nğŸ” ç™¼ç¾ {merged_count} çµ„é‡è¤‡è³‡æ–™ï¼Œä»¥ä¸‹åˆ—å‡ºå‰ 3 çµ„ç¯„ä¾‹ï¼š")
        
        for i, (key, entries) in enumerate(merged_groups.items()):
            if i >= 3: break
            
            conv_id, t_id, c_id = key
            print(f"\n  [ç¯„ä¾‹ {i+1}] Conv: {conv_id} | Target: {t_id} | Cause: {c_id}")
            print(f"  å…± {len(entries)} ç­†åŸå§‹è³‡æ–™è¢«åˆä½µ:")
            
            for ent in entries:
                label_str = "Positive (1)" if ent['label'] == 1 else "Negative (0)"
                print(f"    - Label: {ent['label']} | Span: \"{ent['span']}\"")
                # print(f"      ID: {ent['id']}") # å¦‚æœæƒ³çœ‹åŸå§‹ ID å¯æ‰“é–‹é€™è¡Œ

if __name__ == "__main__":
    for filename in TARGET_FILES:
        file_path = os.path.join(BASE_CSV_DIR, filename)
        analyze_merges(file_path)